{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries\n",
        "\n",
        "The below block imports all needed libraries. Feel free to add additional libraries that you need and rerun below block.\n",
        "\n",
        "Two last lines inform you of the Pytorch version and the availability of GPU.\n",
        "The last line should print `GPU availability: True`."
      ],
      "metadata": {
        "id": "cGIhoTI28mCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4N6cClJOO8dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296f21e6-9a9a-4e1e-8905-d60ab2c4e437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version:  1.12.1+cu113\n",
            "GPU availability:  True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "from io import TextIOWrapper\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.mkdir(\"./results\")\n",
        "if not os.path.exists(\"./vgg16-results\"):\n",
        "  os.mkdir(\"./vgg16-results\")\n",
        "\n",
        "\n",
        "\n",
        "print('Pytorch version: ', torch.__version__)\n",
        "print('GPU availability: ', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset\n",
        "If you are familiar with Linux bash scripts, you can put `!` at the beginning of a command to order Colab of interpreting it as bash scripts instead of python scripts.\n",
        "\n",
        "The below block downloads MNIST dataset and decompresses it."
      ],
      "metadata": {
        "id": "2-K-36199b9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/myleott/mnist_png/raw/master/mnist_png.tar.gz\n",
        "!tar xzf mnist_png.tar.gz"
      ],
      "metadata": {
        "id": "DgCT6iUz_EFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4022d6ed-465b-4f32-9ec9-b47eb156e911"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-02 12:53:13--  https://github.com/myleott/mnist_png/raw/master/mnist_png.tar.gz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/myleott/mnist_png/master/mnist_png.tar.gz [following]\n",
            "--2022-10-02 12:53:13--  https://raw.githubusercontent.com/myleott/mnist_png/master/mnist_png.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15683414 (15M) [application/octet-stream]\n",
            "Saving to: ‘mnist_png.tar.gz’\n",
            "\n",
            "mnist_png.tar.gz    100%[===================>]  14.96M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-10-02 12:53:15 (450 MB/s) - ‘mnist_png.tar.gz’ saved [15683414/15683414]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dataset Class\n",
        "\n",
        "In order to use a dataset, we need to design a pytorch Dataset Class to process it.\n",
        "In the block below, you are required to complete: \n",
        "\n",
        "* **TODO 1**: `def __init__(root, transform)` function to build the MNIST dataset from images included in the `root` directory. Please add code below `TODO1` to complete this function. The dataset should be captured by two lists, i.e., `self.images` that contains all images of MNIST, and `self.labels` that contains the corresponding label of each image in `self.images`.\n",
        "* **TODO 2**: `def __getitem__(index)` to draw a sample at `index` and its corresponding label. This function should return a tuple (X, y), where X is the image (numpy ndarray of shape 1x28x28) and y is a scalar from 0 to 9 representing X's label."
      ],
      "metadata": {
        "id": "UwNWWw6c-EBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Dataset:\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        # `root` is expected to contain 10 sub-directories, each of which is named after the label of all images included inside.\n",
        "        # transform is a Torchvision.Transforms object that pre-processes an image \n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        #TODO 1: Read dataset.\n",
        "\n",
        "        # All images should be contained in a list `self.images`, and their corresponding labels in a list `self.labels`\n",
        "        # `self.images[i]` should contain a numpy ndarrays of size 1x28x28.\n",
        "        # `self.labels[i]` should contain a single integer of [0-9] representing the label of `self.images[i]`.\n",
        "        self.images = list()\n",
        "        self.labels = list()\n",
        "\n",
        "        # Read all directories\n",
        "        for labelDir in os.scandir(root):\n",
        "          label = labelDir.name\n",
        "          for img in os.scandir(os.path.join(root, label)):\n",
        "            self.addImage(label, img.path)\n",
        "         \n",
        "      \n",
        "    def addImage(self, label: str, imgPath: str):\n",
        "      self.labels.append(int(label))\n",
        "      img = Image.open(imgPath)\n",
        "      img = asarray(img)\n",
        "      # img = img[np.newaxis,...]\n",
        "      self.images.append(img)\n",
        "\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # TODO2: retrieve `self.images[index]` and feed the image into self.transform.\n",
        "        # Then, return a tuple (X, y), where X is the image and y is its label.\n",
        "        image = self.images[index]\n",
        "        image = self.transform(image)\n",
        "        label = self.labels[index]\n",
        "        return image, label\n",
        "\n",
        "    def show_random(self):\n",
        "        indices = np.random.randint(0, len(self), [16,])\n",
        "        f, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                ax[i, j].imshow(self.images[indices[i * 4 + j]])\n",
        "                ax[i, j].tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
        "                ax[i, j].set_title(f'Label: {self.labels[indices[i * 4 + j]]}')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "ClXGBsu3PNHx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model Class\n",
        "Below, we define a simple Multi-layer Perceptron Network with a hidden layer.\n",
        "A pytorch model necessarily have two functions, i.e., `__init__`, which defines all layers of the network, and `forward`, which is fed the input data and processes through all layers defined in `__init__`."
      ],
      "metadata": {
        "id": "-tSCAusZQXV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Network:\n",
        "class MLPNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 500)\n",
        "        self.fc2 = nn.Linear(500, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28) # Flatten every image into a single vector\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP\""
      ],
      "metadata": {
        "id": "daCrD96Bgpfr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create MNISTDataset objects and dataloaders \n",
        "Below, we create the objects to process training and testing sets of MNIST data.\n",
        "As there are no held-out validation set, we manually split the training set into training and validation subsets with the ratio of 8:2.\n",
        "\n",
        "After creating dataset objects, we wrap them by a Pytorch Dataloader to allow several necessary features in training deep learning models, e.g., mini-batch feeding, shuffling.\n",
        "\n",
        "***Note***: if you successfully complete `__init__` function of `MNISTDataset`, its `show_random` function would successfully randomly show 16 images and corresponding labels in the dataset."
      ],
      "metadata": {
        "id": "mdCME6xeRMBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# Hyper parameters\n",
        "################################################################\n",
        "BATCH_SIZE = 128\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "\n",
        "################################################################\n",
        "# Create training and testing dataset and show random examples\n",
        "################################################################\n",
        "trainval_set = MNISTDataset('mnist_png/training', transform=transform)\n",
        "trainval_set.show_random()\n",
        "\n",
        "test_set = MNISTDataset('mnist_png/testing', transform=transform)\n",
        "\n",
        "################################################################\n",
        "# As there is no validation set\n",
        "# We split training dataset into training and validation sets\n",
        "################################################################\n",
        "train_size = int(0.8 * len(trainval_set))\n",
        "val_size = len(trainval_set) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(\n",
        "    dataset=trainval_set,\n",
        "    lengths=[train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "################################################################\n",
        "# Print lengths of subsets\n",
        "################################################################\n",
        "print('Training set size: ', len(train_set))\n",
        "print('Validation set size: ', len(val_set))\n",
        "print('Testing set size: ', len(test_set))\n",
        "\n",
        "################################################################\n",
        "# Print lengths of subsets\n",
        "################################################################\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset=val_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "id": "4KgEjy2VEIhx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "fc5eb2e6-6f82-4f5b-f8d0-792f02b0ff5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size:  48000\n",
            "Validation set size:  12000\n",
            "Testing set size:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model and Training Process\n",
        "\n",
        "In the below block, we create an object (our model) from the MLPNet deep neural network defined above. \n",
        "\n",
        "Then, we create the `criterion` that will compute a loss value from predictions genereated by our model and groundtruth labels. We also create the `optimizer`, which updates our model's learnable parameters based on the loss value to improve its performance.\n",
        "\n",
        "Finally, we start training the model through `EPOCHS` number of epochs. At each epoch, after training the model through the training subset, we evaluate its loss and accuracy on validation subset. Usually, we would base on the loss or accuracy on validation subset to pick out the best performed model during our training process.\n",
        "\n",
        "Your tasks:\n",
        "\n",
        "\n",
        "*   **TODO 3**: Based on average accuracy on validation set, save the model weights into a file. \n",
        "\n",
        "    *Hint*: use `torch.save(model.state_dict(), PATH)` to save model weights into a file specified by `PATH`.\n",
        "*   **TODO 4**: Load the best model weights saved in `PATH` from above task into our `model`. Then, compute loss and accuracy of the best model on testing subset. \n",
        "\n",
        "    *Hint*: use `checkpoint = torch.load(PATH)` to load content of file specified in `PATH` into `checkpoint`, then, use `model.load_state_dict(checkpoint)` to load parameters saved in `checkpoint` into `model`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "swcZkufvS9xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# Hyper parameters\n",
        "################################################################\n",
        "LR = 0.001 # learning rate\n",
        "EPOCHS = 100 # number of epochs to train model\n",
        "\n",
        "################################################################\n",
        "# Create model\n",
        "################################################################\n",
        "model = MLPNet().cuda()\n",
        "\n",
        "################################################################\n",
        "# Create optimizer and criterion\n",
        "################################################################\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "################################################################\n",
        "# Training process\n",
        "################################################################\n",
        "for epoch in range(EPOCHS):\n",
        "    # trainning\n",
        "    total_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss = total_loss / len(train_set)\n",
        "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')\n",
        "\n",
        "    # evaluating\n",
        "    correct_cnt, total_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(val_loader):\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)\n",
        "        _, pred_label = torch.max(out, 1)\n",
        "        correct_cnt += (pred_label == target).sum()\n",
        "        # smooth average\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(val_set)\n",
        "    avg_acc = correct_cnt / len(val_set)\n",
        "    print(f'==>>> epoch: {epoch}, val loss: {avg_loss:.6f}, val accuracy: {avg_acc:.6f}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'./results/{avg_acc: .6f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqbyzc2JgIzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5cce85-0ea5-4c4b-807a-dff887040322"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==>>> epoch: 0, train loss: 0.016684\n",
            "==>>> epoch: 0, val loss: 0.013698, val accuracy: 0.605917\n",
            "==>>> epoch: 1, train loss: 0.009956\n",
            "==>>> epoch: 1, val loss: 0.006757, val accuracy: 0.813667\n",
            "==>>> epoch: 2, train loss: 0.005243\n",
            "==>>> epoch: 2, val loss: 0.004100, val accuracy: 0.853083\n",
            "==>>> epoch: 3, train loss: 0.003918\n",
            "==>>> epoch: 3, val loss: 0.003671, val accuracy: 0.876500\n",
            "==>>> epoch: 4, train loss: 0.003364\n",
            "==>>> epoch: 4, val loss: 0.003494, val accuracy: 0.887667\n",
            "==>>> epoch: 5, train loss: 0.003060\n",
            "==>>> epoch: 5, val loss: 0.003329, val accuracy: 0.892583\n",
            "==>>> epoch: 6, train loss: 0.002864\n",
            "==>>> epoch: 6, val loss: 0.004586, val accuracy: 0.897583\n",
            "==>>> epoch: 7, train loss: 0.002723\n",
            "==>>> epoch: 7, val loss: 0.002115, val accuracy: 0.898917\n",
            "==>>> epoch: 8, train loss: 0.002612\n",
            "==>>> epoch: 8, val loss: 0.001703, val accuracy: 0.902667\n",
            "==>>> epoch: 9, train loss: 0.002521\n",
            "==>>> epoch: 9, val loss: 0.003432, val accuracy: 0.904833\n",
            "==>>> epoch: 10, train loss: 0.002444\n",
            "==>>> epoch: 10, val loss: 0.002953, val accuracy: 0.908167\n",
            "==>>> epoch: 11, train loss: 0.002379\n",
            "==>>> epoch: 11, val loss: 0.002518, val accuracy: 0.908833\n",
            "==>>> epoch: 12, train loss: 0.002314\n",
            "==>>> epoch: 12, val loss: 0.003035, val accuracy: 0.912750\n",
            "==>>> epoch: 13, train loss: 0.002257\n",
            "==>>> epoch: 13, val loss: 0.001803, val accuracy: 0.911583\n",
            "==>>> epoch: 14, train loss: 0.002202\n",
            "==>>> epoch: 14, val loss: 0.002092, val accuracy: 0.914250\n",
            "==>>> epoch: 15, train loss: 0.002154\n",
            "==>>> epoch: 15, val loss: 0.002231, val accuracy: 0.916917\n",
            "==>>> epoch: 16, train loss: 0.002106\n",
            "==>>> epoch: 16, val loss: 0.001138, val accuracy: 0.917500\n",
            "==>>> epoch: 17, train loss: 0.002057\n",
            "==>>> epoch: 17, val loss: 0.002378, val accuracy: 0.920333\n",
            "==>>> epoch: 18, train loss: 0.002018\n",
            "==>>> epoch: 18, val loss: 0.002371, val accuracy: 0.921250\n",
            "==>>> epoch: 19, train loss: 0.001972\n",
            "==>>> epoch: 19, val loss: 0.001931, val accuracy: 0.922000\n",
            "==>>> epoch: 20, train loss: 0.001931\n",
            "==>>> epoch: 20, val loss: 0.001207, val accuracy: 0.923833\n",
            "==>>> epoch: 21, train loss: 0.001890\n",
            "==>>> epoch: 21, val loss: 0.001860, val accuracy: 0.925083\n",
            "==>>> epoch: 22, train loss: 0.001847\n",
            "==>>> epoch: 22, val loss: 0.001917, val accuracy: 0.926667\n",
            "==>>> epoch: 23, train loss: 0.001810\n",
            "==>>> epoch: 23, val loss: 0.002089, val accuracy: 0.929167\n",
            "==>>> epoch: 24, train loss: 0.001769\n",
            "==>>> epoch: 24, val loss: 0.001611, val accuracy: 0.930333\n",
            "==>>> epoch: 25, train loss: 0.001733\n",
            "==>>> epoch: 25, val loss: 0.000915, val accuracy: 0.930417\n",
            "==>>> epoch: 26, train loss: 0.001696\n",
            "==>>> epoch: 26, val loss: 0.002994, val accuracy: 0.933667\n",
            "==>>> epoch: 27, train loss: 0.001663\n",
            "==>>> epoch: 27, val loss: 0.001098, val accuracy: 0.933917\n",
            "==>>> epoch: 28, train loss: 0.001624\n",
            "==>>> epoch: 28, val loss: 0.001632, val accuracy: 0.934917\n",
            "==>>> epoch: 29, train loss: 0.001590\n",
            "==>>> epoch: 29, val loss: 0.001466, val accuracy: 0.937833\n",
            "==>>> epoch: 30, train loss: 0.001555\n",
            "==>>> epoch: 30, val loss: 0.000861, val accuracy: 0.938500\n",
            "==>>> epoch: 31, train loss: 0.001521\n",
            "==>>> epoch: 31, val loss: 0.001109, val accuracy: 0.939000\n",
            "==>>> epoch: 32, train loss: 0.001490\n",
            "==>>> epoch: 32, val loss: 0.000584, val accuracy: 0.940583\n",
            "==>>> epoch: 33, train loss: 0.001457\n",
            "==>>> epoch: 33, val loss: 0.001497, val accuracy: 0.942250\n",
            "==>>> epoch: 34, train loss: 0.001427\n",
            "==>>> epoch: 34, val loss: 0.001596, val accuracy: 0.943083\n",
            "==>>> epoch: 35, train loss: 0.001395\n",
            "==>>> epoch: 35, val loss: 0.001044, val accuracy: 0.943667\n",
            "==>>> epoch: 36, train loss: 0.001368\n",
            "==>>> epoch: 36, val loss: 0.001844, val accuracy: 0.945250\n",
            "==>>> epoch: 37, train loss: 0.001340\n",
            "==>>> epoch: 37, val loss: 0.001189, val accuracy: 0.944250\n",
            "==>>> epoch: 38, train loss: 0.001313\n",
            "==>>> epoch: 38, val loss: 0.001725, val accuracy: 0.947333\n",
            "==>>> epoch: 39, train loss: 0.001287\n",
            "==>>> epoch: 39, val loss: 0.001258, val accuracy: 0.947167\n",
            "==>>> epoch: 40, train loss: 0.001260\n",
            "==>>> epoch: 40, val loss: 0.001153, val accuracy: 0.947833\n",
            "==>>> epoch: 41, train loss: 0.001238\n",
            "==>>> epoch: 41, val loss: 0.001219, val accuracy: 0.949250\n",
            "==>>> epoch: 42, train loss: 0.001210\n",
            "==>>> epoch: 42, val loss: 0.001245, val accuracy: 0.950167\n",
            "==>>> epoch: 43, train loss: 0.001188\n",
            "==>>> epoch: 43, val loss: 0.000695, val accuracy: 0.950333\n",
            "==>>> epoch: 44, train loss: 0.001163\n",
            "==>>> epoch: 44, val loss: 0.000569, val accuracy: 0.949750\n",
            "==>>> epoch: 45, train loss: 0.001142\n",
            "==>>> epoch: 45, val loss: 0.000996, val accuracy: 0.951833\n",
            "==>>> epoch: 46, train loss: 0.001119\n",
            "==>>> epoch: 46, val loss: 0.000854, val accuracy: 0.953750\n",
            "==>>> epoch: 47, train loss: 0.001097\n",
            "==>>> epoch: 47, val loss: 0.001403, val accuracy: 0.952750\n",
            "==>>> epoch: 48, train loss: 0.001074\n",
            "==>>> epoch: 48, val loss: 0.001642, val accuracy: 0.954167\n",
            "==>>> epoch: 49, train loss: 0.001057\n",
            "==>>> epoch: 49, val loss: 0.001011, val accuracy: 0.954000\n",
            "==>>> epoch: 50, train loss: 0.001036\n",
            "==>>> epoch: 50, val loss: 0.000921, val accuracy: 0.956667\n",
            "==>>> epoch: 51, train loss: 0.001016\n",
            "==>>> epoch: 51, val loss: 0.001305, val accuracy: 0.956083\n",
            "==>>> epoch: 52, train loss: 0.000998\n",
            "==>>> epoch: 52, val loss: 0.001231, val accuracy: 0.956417\n",
            "==>>> epoch: 53, train loss: 0.000979\n",
            "==>>> epoch: 53, val loss: 0.001164, val accuracy: 0.957667\n",
            "==>>> epoch: 54, train loss: 0.000959\n",
            "==>>> epoch: 54, val loss: 0.001315, val accuracy: 0.956583\n",
            "==>>> epoch: 55, train loss: 0.000943\n",
            "==>>> epoch: 55, val loss: 0.000279, val accuracy: 0.959500\n",
            "==>>> epoch: 56, train loss: 0.000926\n",
            "==>>> epoch: 56, val loss: 0.001192, val accuracy: 0.957250\n",
            "==>>> epoch: 57, train loss: 0.000913\n",
            "==>>> epoch: 57, val loss: 0.000330, val accuracy: 0.959583\n",
            "==>>> epoch: 58, train loss: 0.000892\n",
            "==>>> epoch: 58, val loss: 0.001109, val accuracy: 0.958833\n",
            "==>>> epoch: 59, train loss: 0.000877\n",
            "==>>> epoch: 59, val loss: 0.001355, val accuracy: 0.959667\n",
            "==>>> epoch: 60, train loss: 0.000863\n",
            "==>>> epoch: 60, val loss: 0.000608, val accuracy: 0.960917\n",
            "==>>> epoch: 61, train loss: 0.000847\n",
            "==>>> epoch: 61, val loss: 0.000618, val accuracy: 0.961750\n",
            "==>>> epoch: 62, train loss: 0.000832\n",
            "==>>> epoch: 62, val loss: 0.000975, val accuracy: 0.961750\n",
            "==>>> epoch: 63, train loss: 0.000817\n",
            "==>>> epoch: 63, val loss: 0.000668, val accuracy: 0.962250\n",
            "==>>> epoch: 64, train loss: 0.000804\n",
            "==>>> epoch: 64, val loss: 0.000732, val accuracy: 0.962250\n",
            "==>>> epoch: 65, train loss: 0.000789\n",
            "==>>> epoch: 65, val loss: 0.000766, val accuracy: 0.962250\n",
            "==>>> epoch: 66, train loss: 0.000776\n",
            "==>>> epoch: 66, val loss: 0.000703, val accuracy: 0.962167\n",
            "==>>> epoch: 67, train loss: 0.000765\n",
            "==>>> epoch: 67, val loss: 0.000544, val accuracy: 0.963083\n",
            "==>>> epoch: 68, train loss: 0.000752\n",
            "==>>> epoch: 68, val loss: 0.000910, val accuracy: 0.963833\n",
            "==>>> epoch: 69, train loss: 0.000738\n",
            "==>>> epoch: 69, val loss: 0.000581, val accuracy: 0.963833\n",
            "==>>> epoch: 70, train loss: 0.000725\n",
            "==>>> epoch: 70, val loss: 0.000425, val accuracy: 0.964500\n",
            "==>>> epoch: 71, train loss: 0.000714\n",
            "==>>> epoch: 71, val loss: 0.000909, val accuracy: 0.965333\n",
            "==>>> epoch: 72, train loss: 0.000701\n",
            "==>>> epoch: 72, val loss: 0.000713, val accuracy: 0.965583\n",
            "==>>> epoch: 73, train loss: 0.000690\n",
            "==>>> epoch: 73, val loss: 0.000618, val accuracy: 0.965250\n",
            "==>>> epoch: 74, train loss: 0.000679\n",
            "==>>> epoch: 74, val loss: 0.000910, val accuracy: 0.965583\n",
            "==>>> epoch: 75, train loss: 0.000669\n",
            "==>>> epoch: 75, val loss: 0.000588, val accuracy: 0.965583\n",
            "==>>> epoch: 76, train loss: 0.000658\n",
            "==>>> epoch: 76, val loss: 0.000979, val accuracy: 0.966667\n",
            "==>>> epoch: 77, train loss: 0.000649\n",
            "==>>> epoch: 77, val loss: 0.000716, val accuracy: 0.966000\n",
            "==>>> epoch: 78, train loss: 0.000638\n",
            "==>>> epoch: 78, val loss: 0.000386, val accuracy: 0.967083\n",
            "==>>> epoch: 79, train loss: 0.000628\n",
            "==>>> epoch: 79, val loss: 0.000480, val accuracy: 0.966417\n",
            "==>>> epoch: 80, train loss: 0.000618\n",
            "==>>> epoch: 80, val loss: 0.000227, val accuracy: 0.967250\n",
            "==>>> epoch: 81, train loss: 0.000609\n",
            "==>>> epoch: 81, val loss: 0.000804, val accuracy: 0.967750\n",
            "==>>> epoch: 82, train loss: 0.000599\n",
            "==>>> epoch: 82, val loss: 0.000326, val accuracy: 0.968333\n",
            "==>>> epoch: 83, train loss: 0.000589\n",
            "==>>> epoch: 83, val loss: 0.000890, val accuracy: 0.968250\n",
            "==>>> epoch: 84, train loss: 0.000579\n",
            "==>>> epoch: 84, val loss: 0.000629, val accuracy: 0.968083\n",
            "==>>> epoch: 85, train loss: 0.000573\n",
            "==>>> epoch: 85, val loss: 0.000353, val accuracy: 0.968750\n",
            "==>>> epoch: 86, train loss: 0.000561\n",
            "==>>> epoch: 86, val loss: 0.001053, val accuracy: 0.968000\n",
            "==>>> epoch: 87, train loss: 0.000554\n",
            "==>>> epoch: 87, val loss: 0.000442, val accuracy: 0.968667\n",
            "==>>> epoch: 88, train loss: 0.000544\n",
            "==>>> epoch: 88, val loss: 0.000290, val accuracy: 0.969000\n",
            "==>>> epoch: 89, train loss: 0.000537\n",
            "==>>> epoch: 89, val loss: 0.000512, val accuracy: 0.968750\n",
            "==>>> epoch: 90, train loss: 0.000529\n",
            "==>>> epoch: 90, val loss: 0.000415, val accuracy: 0.969833\n",
            "==>>> epoch: 91, train loss: 0.000522\n",
            "==>>> epoch: 91, val loss: 0.000620, val accuracy: 0.969250\n",
            "==>>> epoch: 92, train loss: 0.000513\n",
            "==>>> epoch: 92, val loss: 0.000643, val accuracy: 0.969583\n",
            "==>>> epoch: 93, train loss: 0.000506\n",
            "==>>> epoch: 93, val loss: 0.000428, val accuracy: 0.968333\n",
            "==>>> epoch: 94, train loss: 0.000497\n",
            "==>>> epoch: 94, val loss: 0.000553, val accuracy: 0.969417\n",
            "==>>> epoch: 95, train loss: 0.000491\n",
            "==>>> epoch: 95, val loss: 0.000283, val accuracy: 0.969833\n",
            "==>>> epoch: 96, train loss: 0.000485\n",
            "==>>> epoch: 96, val loss: 0.000204, val accuracy: 0.970500\n",
            "==>>> epoch: 97, train loss: 0.000477\n",
            "==>>> epoch: 97, val loss: 0.000517, val accuracy: 0.971333\n",
            "==>>> epoch: 98, train loss: 0.000470\n",
            "==>>> epoch: 98, val loss: 0.000611, val accuracy: 0.970500\n",
            "==>>> epoch: 99, train loss: 0.000463\n",
            "==>>> epoch: 99, val loss: 0.000241, val accuracy: 0.971000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# Testing process\n",
        "################################################################\n",
        "# TODO4: use best performed model from the above process to compute loss and accuracy on testing set.\n",
        "\n",
        "def is_float(element) -> bool:\n",
        "    try:\n",
        "        float(element)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# Find best model \n",
        "results = list()\n",
        "for result in os.scandir(\"./results\"):\n",
        "  if is_float(result.name):\n",
        "    results.append(float(result.name))\n",
        "\n",
        "\n",
        "best_result = max(results)\n",
        "\n",
        "best_model_path = os.path.join(os.getcwd(), f'results/ {best_result}')\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "# evaluating\n",
        "correct_cnt, total_loss = 0, 0\n",
        "for batch_idx, (x, target) in enumerate(test_loader):\n",
        "    x, target = x.cuda(), target.cuda()\n",
        "    out = model(x)\n",
        "    _, pred_label = torch.max(out, 1)\n",
        "    correct_cnt += (pred_label == target).sum()\n",
        "    # smooth average\n",
        "    total_loss += loss.item()\n",
        "avg_loss = total_loss / len(test_set)\n",
        "avg_acc = correct_cnt / len(val_set)\n",
        "print(f'loss: {avg_loss:.6f}, accuracy: {avg_acc:.6f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfe50Qk26mX3",
        "outputId": "a931954e-b37d-4072-9cb1-980ec51dafa3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/results/ 0.971333\n",
            "loss: 0.000243, accuracy: 0.809833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Famous State-of-the-art Neural Network on MNIST\n",
        "\n",
        "In the next three blocks, you are requested to find the pytorch implementations for three famous state-of-the-art networks (i.e., LeNet, VGG16, and ResNet18) and train them using the training process similar to the above block.\n",
        "\n",
        "Thes tasks will help you have a comparisons between state-of-the-arts. Your specific tasks are:\n",
        "\n",
        "*   **TODO 5**: Define ***LeNet*** network and train them using *Cross Entropy* loss, *SGD* optimizer, *learning rate* of 0.001, and in 100 *epochs*. Saving best performed model on validation subset during training process, and finally evaluate its performance (loss, accuracy) on testing set.\n",
        "*   **TODO 6**: Define ***VGG16*** network and train t[link text](https://)hem using *Cross Entropy* loss, *SGD* optimizer, *learning rate* of 0.001, and in 100 *epochs*. Saving best performed model on validation subset during training process, and finally evaluate its performance (loss, accuracy) on testing set.\n",
        "* **TODO 7**: Define ***ResNet18*** network and train them using *Cross Entropy* loss, *SGD* optimizer, *learning rate* of 0.001, and in 100 *epochs*. Saving best performed model on validation subset during training process, and finally evaluate its performance (loss, accuracy) on testing set.\n",
        "\n"
      ],
      "metadata": {
        "id": "xyJPBp0hWWu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO5: Define LeNet network and train it using above training and testing processes\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "\n",
        "class lenet(nn.Module):\n",
        "    '''\n",
        "    input: 3x32x32 image\n",
        "    output: 10 class probability\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(lenet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3,6,5)\n",
        "        self.conv2 = nn.Conv2d(6,16,5) #c3:feature_maps 16@10x10\n",
        "        self.maxPool = nn.MaxPool2d(2,2) #subsampling 1/2size\n",
        "        self.fc1 = nn.Linear(16*5*5,120) #f5:layer120\n",
        "        self.fc2 = nn.Linear(120,84) #f6:layer84\n",
        "        self.fc3 = nn.Linear(84,10) #output:10 class\n",
        "\n",
        "    def forward(self,x):\n",
        "        print(x.shape)\n",
        "        x = self.maxPool(func.relu(self.conv1(x)))\n",
        "        x = self.maxPool(func.relu(self.conv2(x)))\n",
        "        print(x.shape)\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = func.relu(self.fc1(x))\n",
        "        x = func.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "################################################################\n",
        "# Hyper parameters\n",
        "################################################################\n",
        "LR = 0.001 # learning rate\n",
        "EPOCHS = 100 # number of epochs to train model\n",
        "\n",
        "################################################################\n",
        "# Create model\n",
        "################################################################\n",
        "model = lenet();\n",
        "\n",
        "################################################################\n",
        "# Create optimizer and criterion\n",
        "################################################################\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "################################################################\n",
        "# Training process\n",
        "################################################################\n",
        "for epoch in range(EPOCHS):\n",
        "    # trainning\n",
        "    total_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss = total_loss / len(train_set)\n",
        "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')\n",
        "\n",
        "    # evaluating\n",
        "    correct_cnt, total_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(val_loader):\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        out = model(x)\n",
        "        _, pred_label = torch.max(out, 1)\n",
        "        correct_cnt += (pred_label == target).sum()\n",
        "        # smooth average\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(val_set)\n",
        "    avg_acc = correct_cnt / len(val_set)\n",
        "    print(f'==>>> epoch: {epoch}, val loss: {avg_loss:.6f}, val accuracy: {avg_acc:.6f}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'./vgg16-results/{avg_acc: .6f}')\n",
        "\n",
        "# Find best model \n",
        "results = list()\n",
        "for result in os.scandir(\"./vgg16-results\"):\n",
        "  if is_float(result.name):\n",
        "    results.append(float(result.name))\n",
        "\n",
        "\n",
        "best_result = max(results)\n",
        "\n",
        "best_model_path = os.path.join(os.getcwd(), f'vgg16-results/ {best_result}')\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "# evaluating\n",
        "correct_cnt, total_loss = 0, 0\n",
        "for batch_idx, (x, target) in enumerate(test_loader):\n",
        "    x = x.repeat(1, 3, 1, 1)\n",
        "    out = model(x)\n",
        "    _, pred_label = torch.max(out, 1)\n",
        "    correct_cnt += (pred_label == target).sum()\n",
        "    # smooth average\n",
        "    total_loss += loss.item()\n",
        "avg_loss = total_loss / len(test_set)\n",
        "avg_acc = correct_cnt / len(val_set)\n",
        "print(f'loss: {avg_loss:.6f}, accuracy: {avg_acc:.6f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9n1JSyVarjXp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "14c15dca-5e12-4765-ebdc-d1660d16733d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 28, 28])\n",
            "torch.Size([128, 16, 4, 4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-632fbf41ff06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-632fbf41ff06>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 400]' is invalid for input of size 32768"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO6: Define VGG16 network and train it using above training and testing processes\n",
        "\n",
        "################################################################\n",
        "# Hyper parameters\n",
        "################################################################\n",
        "LR = 0.001 # learning rate\n",
        "EPOCHS = 100 # number of epochs to train model\n",
        "\n",
        "################################################################\n",
        "# Create model\n",
        "################################################################\n",
        "model = torchvision.models.vgg16();\n",
        "\n",
        "################################################################\n",
        "# Create optimizer and criterion\n",
        "################################################################\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "################################################################\n",
        "# Training process\n",
        "################################################################\n",
        "for epoch in range(EPOCHS):\n",
        "    # trainning\n",
        "    total_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss = total_loss / len(train_set)\n",
        "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')\n",
        "\n",
        "    # evaluating\n",
        "    correct_cnt, total_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(val_loader):\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        out = model(x)\n",
        "        _, pred_label = torch.max(out, 1)\n",
        "        correct_cnt += (pred_label == target).sum()\n",
        "        # smooth average\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(val_set)\n",
        "    avg_acc = correct_cnt / len(val_set)\n",
        "    print(f'==>>> epoch: {epoch}, val loss: {avg_loss:.6f}, val accuracy: {avg_acc:.6f}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'./vgg16-results/{avg_acc: .6f}')\n",
        "\n",
        "# Find best model \n",
        "results = list()\n",
        "for result in os.scandir(\"./vgg16-results\"):\n",
        "  if is_float(result.name):\n",
        "    results.append(float(result.name))\n",
        "\n",
        "\n",
        "best_result = max(results)\n",
        "\n",
        "best_model_path = os.path.join(os.getcwd(), f'vgg16-results/ {best_result}')\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "# evaluating\n",
        "correct_cnt, total_loss = 0, 0\n",
        "for batch_idx, (x, target) in enumerate(test_loader):\n",
        "    x = x.repeat(1, 3, 1, 1)\n",
        "    out = model(x)\n",
        "    _, pred_label = torch.max(out, 1)\n",
        "    correct_cnt += (pred_label == target).sum()\n",
        "    # smooth average\n",
        "    total_loss += loss.item()\n",
        "avg_loss = total_loss / len(test_set)\n",
        "avg_acc = correct_cnt / len(val_set)\n",
        "print(f'loss: {avg_loss:.6f}, accuracy: {avg_acc:.6f}')"
      ],
      "metadata": {
        "id": "p3N6CQhrrwjO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "588182d6-3183-46fd-93c2-8bf5c123c656"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-96aa10bcff10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                             return_indices=self.return_indices)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO7: Define Resnet18 network and train it using above training and testing processes\n",
        "\n",
        "################################################################\n",
        "# Hyper parameters\n",
        "################################################################\n",
        "LR = 0.001 # learning rate\n",
        "EPOCHS = 100 # number of epochs to train model\n",
        "\n",
        "################################################################\n",
        "# Create model\n",
        "################################################################\n",
        "model = torchvision.models.resnet18();\n",
        "\n",
        "################################################################\n",
        "# Create optimizer and criterion\n",
        "################################################################\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "################################################################\n",
        "# Training process\n",
        "################################################################\n",
        "for epoch in range(EPOCHS):\n",
        "    # trainning\n",
        "    total_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x, target\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss = total_loss / len(train_set)\n",
        "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')\n",
        "\n",
        "    # evaluating\n",
        "    correct_cnt, total_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(val_loader):\n",
        "        x, target = x, target\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        out = model(x)\n",
        "        _, pred_label = torch.max(out, 1)\n",
        "        correct_cnt += (pred_label == target).sum()\n",
        "        # smooth average\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(val_set)\n",
        "    avg_acc = correct_cnt / len(val_set)\n",
        "    print(f'==>>> epoch: {epoch}, val loss: {avg_loss:.6f}, val accuracy: {avg_acc:.6f}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'./vgg16-results/{avg_acc: .6f}')\n",
        "\n",
        "# Find best model \n",
        "results = list()\n",
        "for result in os.scandir(\"./vgg16-results\"):\n",
        "  if is_float(result.name):\n",
        "    results.append(float(result.name))\n",
        "\n",
        "\n",
        "best_result = max(results)\n",
        "\n",
        "best_model_path = os.path.join(os.getcwd(), f'vgg16-results/ {best_result}')\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "# evaluating\n",
        "correct_cnt, total_loss = 0, 0\n",
        "for batch_idx, (x, target) in enumerate(test_loader):\n",
        "    x, target = x, target\n",
        "    x = x.repeat(1, 3, 1, 1)\n",
        "    out = model(x)\n",
        "    _, pred_label = torch.max(out, 1)\n",
        "    correct_cnt += (pred_label == target).sum()\n",
        "    # smooth average\n",
        "    total_loss += loss.item()\n",
        "avg_loss = total_loss / len(test_set)\n",
        "avg_acc = correct_cnt / len(val_set)\n",
        "print(f'loss: {avg_loss:.6f}, accuracy: {avg_acc:.6f}')"
      ],
      "metadata": {
        "id": "nB9wg24WruaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4034d9-1dc9-4561-c0c7-9fb2f3447b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==>>> epoch: 0, train loss: 0.003244\n",
            "==>>> epoch: 0, val loss: 0.000668, val accuracy: 0.973583\n",
            "==>>> epoch: 1, train loss: 0.000424\n",
            "==>>> epoch: 1, val loss: 0.000357, val accuracy: 0.981417\n",
            "==>>> epoch: 2, train loss: 0.000225\n",
            "==>>> epoch: 2, val loss: 0.000162, val accuracy: 0.980833\n",
            "==>>> epoch: 3, train loss: 0.000133\n",
            "==>>> epoch: 3, val loss: 0.000134, val accuracy: 0.983583\n",
            "==>>> epoch: 4, train loss: 0.000068\n",
            "==>>> epoch: 4, val loss: 0.000059, val accuracy: 0.983917\n",
            "==>>> epoch: 5, train loss: 0.000046\n",
            "==>>> epoch: 5, val loss: 0.000102, val accuracy: 0.984667\n",
            "==>>> epoch: 6, train loss: 0.000032\n",
            "==>>> epoch: 6, val loss: 0.000017, val accuracy: 0.984750\n",
            "==>>> epoch: 7, train loss: 0.000024\n",
            "==>>> epoch: 7, val loss: 0.000004, val accuracy: 0.985417\n",
            "==>>> epoch: 8, train loss: 0.000018\n",
            "==>>> epoch: 8, val loss: 0.000048, val accuracy: 0.986583\n",
            "==>>> epoch: 9, train loss: 0.000015\n",
            "==>>> epoch: 9, val loss: 0.000013, val accuracy: 0.986333\n",
            "==>>> epoch: 10, train loss: 0.000012\n",
            "==>>> epoch: 10, val loss: 0.000010, val accuracy: 0.986000\n",
            "==>>> epoch: 11, train loss: 0.000011\n",
            "==>>> epoch: 11, val loss: 0.000009, val accuracy: 0.986083\n",
            "==>>> epoch: 12, train loss: 0.000010\n",
            "==>>> epoch: 12, val loss: 0.000004, val accuracy: 0.987083\n",
            "==>>> epoch: 13, train loss: 0.000008\n",
            "==>>> epoch: 13, val loss: 0.000005, val accuracy: 0.986333\n",
            "==>>> epoch: 14, train loss: 0.000008\n",
            "==>>> epoch: 14, val loss: 0.000008, val accuracy: 0.986833\n",
            "==>>> epoch: 15, train loss: 0.000007\n",
            "==>>> epoch: 15, val loss: 0.000003, val accuracy: 0.985333\n",
            "==>>> epoch: 16, train loss: 0.000006\n",
            "==>>> epoch: 16, val loss: 0.000005, val accuracy: 0.986000\n",
            "==>>> epoch: 17, train loss: 0.000006\n",
            "==>>> epoch: 17, val loss: 0.000008, val accuracy: 0.986333\n",
            "==>>> epoch: 18, train loss: 0.000005\n",
            "==>>> epoch: 18, val loss: 0.000003, val accuracy: 0.986167\n",
            "==>>> epoch: 19, train loss: 0.000005\n",
            "==>>> epoch: 19, val loss: 0.000004, val accuracy: 0.986833\n",
            "==>>> epoch: 20, train loss: 0.000005\n",
            "==>>> epoch: 20, val loss: 0.000003, val accuracy: 0.986417\n",
            "==>>> epoch: 21, train loss: 0.000005\n",
            "==>>> epoch: 21, val loss: 0.000002, val accuracy: 0.986417\n",
            "==>>> epoch: 22, train loss: 0.000004\n",
            "==>>> epoch: 22, val loss: 0.000002, val accuracy: 0.986500\n",
            "==>>> epoch: 23, train loss: 0.000004\n",
            "==>>> epoch: 23, val loss: 0.000001, val accuracy: 0.986917\n",
            "==>>> epoch: 24, train loss: 0.000004\n",
            "==>>> epoch: 24, val loss: 0.000002, val accuracy: 0.986917\n",
            "==>>> epoch: 25, train loss: 0.000004\n",
            "==>>> epoch: 25, val loss: 0.000003, val accuracy: 0.986500\n",
            "==>>> epoch: 26, train loss: 0.000003\n",
            "==>>> epoch: 26, val loss: 0.000002, val accuracy: 0.986583\n",
            "==>>> epoch: 27, train loss: 0.000003\n",
            "==>>> epoch: 27, val loss: 0.000005, val accuracy: 0.986833\n",
            "==>>> epoch: 28, train loss: 0.000003\n",
            "==>>> epoch: 28, val loss: 0.000002, val accuracy: 0.986750\n",
            "==>>> epoch: 29, train loss: 0.000003\n",
            "==>>> epoch: 29, val loss: 0.000003, val accuracy: 0.987000\n",
            "==>>> epoch: 30, train loss: 0.000003\n",
            "==>>> epoch: 30, val loss: 0.000006, val accuracy: 0.986833\n",
            "==>>> epoch: 31, train loss: 0.000003\n",
            "==>>> epoch: 31, val loss: 0.000004, val accuracy: 0.987000\n",
            "==>>> epoch: 32, train loss: 0.000002\n",
            "==>>> epoch: 32, val loss: 0.000001, val accuracy: 0.986917\n",
            "==>>> epoch: 33, train loss: 0.000002\n",
            "==>>> epoch: 33, val loss: 0.000001, val accuracy: 0.986833\n",
            "==>>> epoch: 34, train loss: 0.000003\n",
            "==>>> epoch: 34, val loss: 0.000001, val accuracy: 0.987000\n",
            "==>>> epoch: 35, train loss: 0.000002\n",
            "==>>> epoch: 35, val loss: 0.000003, val accuracy: 0.986583\n",
            "==>>> epoch: 36, train loss: 0.000002\n",
            "==>>> epoch: 36, val loss: 0.000005, val accuracy: 0.986750\n",
            "==>>> epoch: 37, train loss: 0.000002\n",
            "==>>> epoch: 37, val loss: 0.000001, val accuracy: 0.986583\n",
            "==>>> epoch: 38, train loss: 0.000002\n",
            "==>>> epoch: 38, val loss: 0.000001, val accuracy: 0.987000\n",
            "==>>> epoch: 39, train loss: 0.000002\n",
            "==>>> epoch: 39, val loss: 0.000002, val accuracy: 0.987083\n"
          ]
        }
      ]
    }
  ]
}