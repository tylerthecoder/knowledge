{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def make_random_bin(length):\n",
    "\t\treturn \"\".join([str(random.randint(0, 1)) for i in range(length)])\n",
    "\n",
    "def add_bin(a: str, b: str):\n",
    "\t\tans = bin(int(a[::-1], 2) + int(b[::-1], 2))[2:]\n",
    "\t\t# Pad with zeros\n",
    "\t\tans = \"0\" * (len(a) - len(ans)) + ans\n",
    "\t\treturn ans[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(Dataset):\n",
    "\tdef __init__(self, seq_length, num_bits):\n",
    "\t\tself.seq_length = seq_length\n",
    "\t\tself.num_bits = num_bits\n",
    "\t\tself.seq1 = make_random_bin(seq_length)\n",
    "\t\tself.seq2 = make_random_bin(seq_length)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tsample1 = self.seq1[index:index + self.num_bits - 1] + \"0\"\n",
    "\t\tsample2 = self.seq2[index:index + self.num_bits - 1] + \"0\"\n",
    "\t\tinput = torch.tensor([[int(x, 2) for x in sample1], [int(x, 2) for x in sample2]])\n",
    "\t\tinput = torch.transpose(input, 0, 1).float()\n",
    "\t\toutput = torch.tensor([int(x, 2) for x in add_bin(sample1, sample2)])\n",
    "\t\treturn input, output\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.seq_length - self.num_bits + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 0.]]), tensor([0, 1, 1, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "data = BinaryDataset(128, 5)\n",
    "test_dataloader = DataLoader(dataset=data, batch_size=128)\n",
    "print(data.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class Model(nn.Module):\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_layers = 1\n",
    "\t\tself.hidden_dim = 10\n",
    "\n",
    "\t\tself.rnn = nn.RNN(input_size=2, hidden_size=self.hidden_dim, num_layers=self.n_layers, nonlinearity='relu')\n",
    "\t\tself.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size = x.size(1)\n",
    "\t\thidden = self.init_hidden(batch_size)\n",
    "\n",
    "\t\t# X is of size (batch_size, sequence_length, input_size)\n",
    "\t\t# hidden is of size (n_layers, sequence_length, hidden_dim)\n",
    "\t\t\n",
    "\t\tout, hidden = self.rnn(x, hidden)\n",
    "\n",
    "\t\tout = self.fc(out)\n",
    "\n",
    "\t\tout = nn.Sigmoid()(out)\n",
    "\n",
    "\t\treturn out, hidden\n",
    "\n",
    "\tdef init_hidden(self, batch_size):\n",
    "\t\t# This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "\t\t# We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "\t\thidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "\t\treturn hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49193548387096775\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Model()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Test the model's accuracy\n",
    "def compute_accuracy(label, pred):\n",
    "\tpred = torch.round(pred).squeeze()\n",
    "\t# Pred is of size (batch_size, sequence_length)\n",
    "\t# Label is of size (batch_size, sequence_length)\n",
    "\treturn torch.sum(label == pred).item() / (len(label) * len(label[0]))\n",
    "\n",
    "\n",
    "# Try the model on random data\n",
    "for x, label in iter(test_dataloader):\n",
    "\tout, hidden = model(x)\n",
    "\tprint(compute_accuracy(label, out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.6979, Accuracy: 47.26%\n",
      "Epoch [1/100], Loss: 0.6955, Accuracy: 47.26%\n",
      "Epoch [2/100], Loss: 0.6937, Accuracy: 49.68%\n",
      "Epoch [3/100], Loss: 0.6924, Accuracy: 51.13%\n",
      "Epoch [4/100], Loss: 0.6915, Accuracy: 59.19%\n",
      "Epoch [5/100], Loss: 0.6906, Accuracy: 52.74%\n",
      "Epoch [6/100], Loss: 0.6900, Accuracy: 52.74%\n",
      "Epoch [7/100], Loss: 0.6895, Accuracy: 52.74%\n",
      "Epoch [8/100], Loss: 0.6891, Accuracy: 52.74%\n",
      "Epoch [9/100], Loss: 0.6888, Accuracy: 52.74%\n",
      "Epoch [10/100], Loss: 0.6886, Accuracy: 52.74%\n",
      "Epoch [11/100], Loss: 0.6884, Accuracy: 52.74%\n",
      "Epoch [12/100], Loss: 0.6882, Accuracy: 52.74%\n",
      "Epoch [13/100], Loss: 0.6880, Accuracy: 52.74%\n",
      "Epoch [14/100], Loss: 0.6878, Accuracy: 52.74%\n",
      "Epoch [15/100], Loss: 0.6875, Accuracy: 52.74%\n",
      "Epoch [16/100], Loss: 0.6872, Accuracy: 52.74%\n",
      "Epoch [17/100], Loss: 0.6869, Accuracy: 52.74%\n",
      "Epoch [18/100], Loss: 0.6864, Accuracy: 52.74%\n",
      "Epoch [19/100], Loss: 0.6860, Accuracy: 52.74%\n",
      "Epoch [20/100], Loss: 0.6855, Accuracy: 52.74%\n",
      "Epoch [21/100], Loss: 0.6849, Accuracy: 52.74%\n",
      "Epoch [22/100], Loss: 0.6843, Accuracy: 52.74%\n",
      "Epoch [23/100], Loss: 0.6837, Accuracy: 53.39%\n",
      "Epoch [24/100], Loss: 0.6831, Accuracy: 56.94%\n",
      "Epoch [25/100], Loss: 0.6824, Accuracy: 59.84%\n",
      "Epoch [26/100], Loss: 0.6818, Accuracy: 60.48%\n",
      "Epoch [27/100], Loss: 0.6811, Accuracy: 61.13%\n",
      "Epoch [28/100], Loss: 0.6803, Accuracy: 61.77%\n",
      "Epoch [29/100], Loss: 0.6793, Accuracy: 61.77%\n",
      "Epoch [30/100], Loss: 0.6781, Accuracy: 61.77%\n",
      "Epoch [31/100], Loss: 0.6767, Accuracy: 61.29%\n",
      "Epoch [32/100], Loss: 0.6752, Accuracy: 60.81%\n",
      "Epoch [33/100], Loss: 0.6734, Accuracy: 59.19%\n",
      "Epoch [34/100], Loss: 0.6714, Accuracy: 58.71%\n",
      "Epoch [35/100], Loss: 0.6696, Accuracy: 58.06%\n",
      "Epoch [36/100], Loss: 0.6681, Accuracy: 60.32%\n",
      "Epoch [37/100], Loss: 0.6675, Accuracy: 58.71%\n",
      "Epoch [38/100], Loss: 0.6668, Accuracy: 59.19%\n",
      "Epoch [39/100], Loss: 0.6661, Accuracy: 56.61%\n",
      "Epoch [40/100], Loss: 0.6650, Accuracy: 56.61%\n",
      "Epoch [41/100], Loss: 0.6635, Accuracy: 58.55%\n",
      "Epoch [42/100], Loss: 0.6623, Accuracy: 62.26%\n",
      "Epoch [43/100], Loss: 0.6615, Accuracy: 61.61%\n",
      "Epoch [44/100], Loss: 0.6612, Accuracy: 60.97%\n",
      "Epoch [45/100], Loss: 0.6608, Accuracy: 61.13%\n",
      "Epoch [46/100], Loss: 0.6602, Accuracy: 60.48%\n",
      "Epoch [47/100], Loss: 0.6594, Accuracy: 61.29%\n",
      "Epoch [48/100], Loss: 0.6584, Accuracy: 61.94%\n",
      "Epoch [49/100], Loss: 0.6576, Accuracy: 63.23%\n",
      "Epoch [50/100], Loss: 0.6568, Accuracy: 63.23%\n",
      "Epoch [51/100], Loss: 0.6557, Accuracy: 63.39%\n",
      "Epoch [52/100], Loss: 0.6547, Accuracy: 63.23%\n",
      "Epoch [53/100], Loss: 0.6539, Accuracy: 63.55%\n",
      "Epoch [54/100], Loss: 0.6529, Accuracy: 63.71%\n",
      "Epoch [55/100], Loss: 0.6514, Accuracy: 64.35%\n",
      "Epoch [56/100], Loss: 0.6500, Accuracy: 65.16%\n",
      "Epoch [57/100], Loss: 0.6488, Accuracy: 65.00%\n",
      "Epoch [58/100], Loss: 0.6474, Accuracy: 65.16%\n",
      "Epoch [59/100], Loss: 0.6460, Accuracy: 64.68%\n",
      "Epoch [60/100], Loss: 0.6449, Accuracy: 64.35%\n",
      "Epoch [61/100], Loss: 0.6432, Accuracy: 65.00%\n",
      "Epoch [62/100], Loss: 0.6421, Accuracy: 64.03%\n",
      "Epoch [63/100], Loss: 0.6406, Accuracy: 64.68%\n",
      "Epoch [64/100], Loss: 0.6394, Accuracy: 64.03%\n",
      "Epoch [65/100], Loss: 0.6378, Accuracy: 64.03%\n",
      "Epoch [66/100], Loss: 0.6360, Accuracy: 65.32%\n",
      "Epoch [67/100], Loss: 0.6345, Accuracy: 66.29%\n",
      "Epoch [68/100], Loss: 0.6332, Accuracy: 65.97%\n",
      "Epoch [69/100], Loss: 0.6321, Accuracy: 65.97%\n",
      "Epoch [70/100], Loss: 0.6306, Accuracy: 66.29%\n",
      "Epoch [71/100], Loss: 0.6291, Accuracy: 67.10%\n",
      "Epoch [72/100], Loss: 0.6276, Accuracy: 66.77%\n",
      "Epoch [73/100], Loss: 0.6262, Accuracy: 67.42%\n",
      "Epoch [74/100], Loss: 0.6247, Accuracy: 67.10%\n",
      "Epoch [75/100], Loss: 0.6233, Accuracy: 67.58%\n",
      "Epoch [76/100], Loss: 0.6217, Accuracy: 66.61%\n",
      "Epoch [77/100], Loss: 0.6202, Accuracy: 66.61%\n",
      "Epoch [78/100], Loss: 0.6190, Accuracy: 67.10%\n",
      "Epoch [79/100], Loss: 0.6176, Accuracy: 66.94%\n",
      "Epoch [80/100], Loss: 0.6167, Accuracy: 67.10%\n",
      "Epoch [81/100], Loss: 0.6154, Accuracy: 67.42%\n",
      "Epoch [82/100], Loss: 0.6148, Accuracy: 66.77%\n",
      "Epoch [83/100], Loss: 0.6127, Accuracy: 67.10%\n",
      "Epoch [84/100], Loss: 0.6113, Accuracy: 68.23%\n",
      "Epoch [85/100], Loss: 0.6090, Accuracy: 68.87%\n",
      "Epoch [86/100], Loss: 0.6069, Accuracy: 68.87%\n",
      "Epoch [87/100], Loss: 0.6057, Accuracy: 68.55%\n",
      "Epoch [88/100], Loss: 0.6044, Accuracy: 69.03%\n",
      "Epoch [89/100], Loss: 0.6034, Accuracy: 69.19%\n",
      "Epoch [90/100], Loss: 0.6019, Accuracy: 68.23%\n",
      "Epoch [91/100], Loss: 0.6005, Accuracy: 67.26%\n",
      "Epoch [92/100], Loss: 0.5994, Accuracy: 67.26%\n",
      "Epoch [93/100], Loss: 0.5975, Accuracy: 67.58%\n",
      "Epoch [94/100], Loss: 0.5969, Accuracy: 67.90%\n",
      "Epoch [95/100], Loss: 0.5958, Accuracy: 67.58%\n",
      "Epoch [96/100], Loss: 0.5947, Accuracy: 67.10%\n",
      "Epoch [97/100], Loss: 0.5934, Accuracy: 67.10%\n",
      "Epoch [98/100], Loss: 0.5926, Accuracy: 68.23%\n",
      "Epoch [99/100], Loss: 0.5914, Accuracy: 70.65%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "best_loss = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\tfor x, label in iter(test_dataloader):\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# Forward pass\n",
    "\t\tout, hidden = model(x)\n",
    "\t\tloss = criterion(out.squeeze(), label.float())\n",
    "\n",
    "\t\t# Backward and optimize\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Compute accuracy\n",
    "\t\taccuracy = compute_accuracy(label, out)\n",
    "\n",
    "\t\tif loss.item() < best_loss:\n",
    "\t\t\tbest_loss = loss.item()\n",
    "\t\t\ttorch.save(model.state_dict(), \"best-model.bin\")\n",
    "\n",
    "\t\tprint(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 3 = 5 ( 100.0 % correct)\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to add two user predicted numbers\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.load_state_dict(torch.load(\"best-model.bin\"))\n",
    "\n",
    "# Get two numbers from the user\n",
    "n1 = int(input(\"Enter the first number: \"))\n",
    "n2 = int(input(\"Enter the second number: \"))\n",
    "\n",
    "# Convert the numbers to binary\n",
    "num1 = bin(n1)[2:]\n",
    "num2 = bin(n2)[2:]\n",
    "\n",
    "# Pad the numbers with zeros so that they are of the same length\n",
    "if len(num1) > len(num2):\n",
    "    num2 = \"0\" * (len(num1) - len(num2)) + num2\n",
    "elif len(num2) > len(num1):\n",
    "    num1 = \"0\" * (len(num2) - len(num1)) + num1\n",
    "\n",
    "# Reverse the numbers\n",
    "num1 = num1[::-1] + \"0\"\n",
    "num2 = num2[::-1] + \"0\"\n",
    "\n",
    "numsT = torch.tensor([[int(x, 2) for x in num1], [int(x, 2) for x in num1]])\n",
    "\n",
    "numsT = numsT.unsqueeze(0).float().transpose(1, 2)\n",
    "\n",
    "out, hidden = model(numsT)\n",
    "\n",
    "# round the output\n",
    "out = torch.round(out).squeeze()\n",
    "\n",
    "# Convert the output to a string\n",
    "outbin = [str(int(x)) for x in out]\n",
    "\n",
    "# Reverse the output\n",
    "out = outbin[::-1]\n",
    "\n",
    "# Convert the output to a number\n",
    "out = int(\"\".join(out), 2)\n",
    "\n",
    "sum = n1 + n2\n",
    "\n",
    "# Convert sum to binary\n",
    "sum = bin(sum)[2:]\n",
    "\n",
    "# reverse the sum\n",
    "sum = sum[::-1]\n",
    "\n",
    "# Compute percent diff between the sum and the model's output\n",
    "# By counting the number of digits correct\n",
    "correct = 0\n",
    "for i in range(len(sum)):\n",
    "    if sum[i] == outbin[i]:\n",
    "        correct += 1\n",
    "percent_correct = correct / len(sum)\n",
    "\n",
    "print(n1, \"+\", n2, \"=\", out, \"(\", percent_correct * 100, \"% correct)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
