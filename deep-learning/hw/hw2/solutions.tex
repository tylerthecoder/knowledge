\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{braket}
\usepackage{amsmath}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%
\usepackage{graphicx}

\begin{document}

\section{Question 1}

What is the difference between CNNs and RNNs?

CNNs can be used to classify data that has spatial structure, such as images. It does this by using filters to extract features from patches of the images that are close to each other. RNNs can be used to classify data that has temporal structure, such as text. It does this by using recurrent connections to extract features from words that are close to each other in the sequence.


What tasks that CNNs can do but RNNs? Please explain with examples.

A CNN can be used to extract features from images whereas that would be near impossible with an RNN. You could feed each pixel into the RNN but that would be very slow and not very efficient.



\section{Question 2}

What are the limitations of RNNs?

RNNs are very slow to train and are not very efficient. They also have a hard time remembering long term dependencies. They are only really good at performing short term predictive tasks (like predicting the next word in a sentence).

If we have a GPU to accelerate speed by parallel computations, can it parallelize the computations of RNN at timestep-level during training phase? If so, does it also hold true at the inference phase of RNN?  Please justify your answer.

Forward and backwards propagation are both sequential. This is because when passing time series data through the network, information is propagated forward. This means you must compute the output of each layer before you can compute the output of the next layer. This is also true for backwards propagation. This means that the GPU cannot parallelize the computations of RNN at timestep-level during training phase. This is also true at the inference phase of RNN.


Explain the problems of vanishing gradient and exploding gradient, in which cases will any of those problems happen?

The gradient tends to get smaller or larger as time goes on. As more layers are processed the gradient will vanish or explode as the gradient gets more extreme. This happens when the activation function has a large variance between their inputs and outputs. Thus the gradient calculation will have a small error.



RNN schemas

The first one is called "One to Many". It is used when multiples outputs are needed for a single input. For example, when you want to produce music off of a single note.

The second one is a "many to one" RNN schema. It is used when multiple inputs yeild a single output. This can be used in NLP to predict the sentiment of a sentence by passing in the sentence word by word and getting a single output of the sentiment of the sentence.

The third one is a "Many to Many" RNN scheam where there are more hidden layers than inputs. This is used when you want to translate a sentence from one language to another. You would pass in the sentence word by word and get the translated sentence word by word. This model has to "memorize" things since it doens't make a prediction until it has proccessed a lot of the input sequence

The fourth one is also a "Many to Many" RNN schema. The difference is this one makes a prediction at each time step. This is used when you want to predict the next word in a sentence. You would pass in the sentence word by word and get the next word in the sentence word by word.

\section{Question 3}

Distinguish between Seq2Seq, attention, self-attention, transformer.

Seq2Seq is a model that converts a sequence of inputs to a sequence of outputs. The length of inputs and outputs do no thave to be the same. Most of the time the sequence is encoded by an encoder which compresses the sequence to a single vector. Then the vector is passed to a decoder RNN which outputs the new sequence. This can be used for language translation.

Attention is used to help the model focus on the important parts of the input sequence. At each decoder step the model gets to decide which part of the input sequence to focus on by learning an attention function. This helps in Seq2Seq model because it allows earlier parts of the input sequence to be used to predict later parts of the output sequence.


Self Attention is the idea of paying attention to the current sequence you are processing. It interacts with elements of the same nature (e.g. the input sequence tokens). Each token is is combined with all of the other tokens in the sequence to form a new state for the token. This can be repeated N times.


Transformers are a model that uses attention for the encoder, decoder, and the interaction between them. The encoder uses self-attention to encode the states. Then the decoder uses "masked" self-attention to predict the next token. The self-attention is masked because it cannot look at the future tokens.


What is multiple head positional encoding used in transformer?



\section{Question 4}


I successfully trained a model to perform binary addition for part 1 and made a python notebook to run it. Here are sample outputs from the deployment procedure.

\begin{verbatim*}
Bin 1 0100 Bin 2 1010
0100 + 1010 = 1110
2 + 5 = 7 ( 100.0 % correct)
\end{verbatim*}

\begin{verbatim*}
Bin 1 010100 Bin 2 001010
010100 + 001010 = 011110
10 + 20 = 30 ( 100.0 % correct)
\end{verbatim*}

\begin{verbatim*}
Bin 1 01111000 Bin 2 11011110
01111000 + 11011110 = 10011001
30 + 123 = 153 ( 100.0 % correct)
\end{verbatim*}


I couldn't get part 2 working. The builtin RNN module with pytorch didn't seem extendable enough. I tried a lot of different methods.




\end{document}
